{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from autowoe import AutoWoE, ReportDeco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение и подготовка обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    \"./data/train_demo.csv\", low_memory=False, index_col=\"line_id\", parse_dates=[\"datetime_\" + str(i) for i in range(2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение и подготовка тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./data/test_demo.csv\", index_col=\"line_id\", parse_dates=[\"datetime_\" + str(i) for i in range(2)])\n",
    "\n",
    "test_target = pd.read_csv(\"./data/test-target_demo.csv\")[\"target\"]\n",
    "test[\"target\"] = test_target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели рекомендуется указать тип признаков для обучения.\n",
    "Поэтому создается словарь features_type с ключами: \n",
    "\n",
    " \"real\" -- вещественный признак\n",
    " \n",
    " \"cat\" --  категориальный.\n",
    " \n",
    " __\"date\"-- (\"%Y%d%m\", (\"m\", \"d\", \"wd\", \"h\", \"min\"))__\n",
    " \n",
    " Для признаков, которые не размечены, типы будут определены автоматом. Такой вариант будет работать, но качество порядочно просядет\n",
    " \n",
    "__Попробуем указать даты с форматом None (автоопределение) и сезонностью - день месяца и день недели__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = list(filter(lambda x: \"numb\" in x, train.columns))\n",
    "num_feature_type = dict.fromkeys(num_col, \"real\")\n",
    "\n",
    "date_col = list(filter(lambda x: \"datetime\" in x, train.columns))\n",
    "date_feature_type = dict.fromkeys(date_col, (None, (\"d\", \"wd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_type = dict(**num_feature_type, **date_feature_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подробно параметры описаны в Example_1\n",
    "auto_woe = AutoWoE(\n",
    "    monotonic=True, max_bin_count=4, oof_woe=False, regularized_refit=False, p_val=0.05, debug=False, verbose=0\n",
    ")\n",
    "auto_woe = ReportDeco(auto_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 63, number of negative: 5537\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11532\n",
      "[LightGBM] [Info] Number of data points in the train set: 5600, number of used features: 652\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011250 -> initscore=-4.476073\n",
      "[LightGBM] [Info] Start training from score -4.476073\n"
     ]
    }
   ],
   "source": [
    "auto_woe.fit(train[num_col + date_col + [\"target\"]], target_name=\"target\", features_type=features_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911446119486321"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = auto_woe.predict_proba(test)\n",
    "roc_auc_score(test[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Замечание\n",
    "ReportDeco - обертка для построения отчета. Она не обязательна для обучения и применения модели, но обязательна для построения отчета (см последнюю ячейку)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Значения коэфициентов и p-values\n",
    "\n",
    "При указании regularized_refit=False будет произведена оценка p-value на коэфициенты модели. Коэфициенты с p-value выше указанного порога не будут включены в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_254         -0.487530\n",
       "number_10          -0.475665\n",
       "number_345         -0.707849\n",
       "number_759         -0.763258\n",
       "number_761         -0.894294\n",
       "number_706         -0.648337\n",
       "number_1           -1.044868\n",
       "number_368         -1.062441\n",
       "datetime_1__F__d   -1.232442\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_woe.features_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.545016720125766"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_woe.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_254          0.013034\n",
       "number_10           0.030010\n",
       "number_345          0.004663\n",
       "number_759          0.001166\n",
       "number_761          0.000357\n",
       "number_706          0.006792\n",
       "number_1            0.001364\n",
       "number_368          0.000006\n",
       "datetime_1__F__d    0.003993\n",
       "Intercept_          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_woe.p_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование отчета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "report_params = {\n",
    "    \"automl_date_column\": \"report_month\",  # колонка с датой в формате params['datetimeFormat']\n",
    "    \"output_path\": \"./AUTOWOE_REPORT_2\",  # папка, куда сгенерится отчет и сложатся нужные файлы\n",
    "    \"report_name\": \"___НАЗВАНИЕ ОТЧЕТА___\",\n",
    "    \"report_version_id\": 1,\n",
    "    \"city\": \"Воронеж\",\n",
    "    \"model_aim\": \"___ЦЕЛЬ ПОСТРОЕНИЯ МОДЕЛИ___\",\n",
    "    \"model_name\": \"___НАЗВАНИЕ МОДЕЛИ___\",\n",
    "    \"zakazchik\": \"___ЗАКАЗЧИК___\",\n",
    "    \"high_level_department\": \"___ПОДРАЗДЕЛЕНИЕ___\",\n",
    "    \"ds_name\": \"___РАЗРАБОТЧИК МОДЕЛИ___\",\n",
    "    \"target_descr\": \"___ОПИСАНИЕ ЦЕЛЕВОГО СОБЫТИЯ___\",\n",
    "    \"non_target_descr\": \"___ОПИСАНИЕ НЕЦЕЛЕВОГО СОБЫТИЯ___\",\n",
    "}\n",
    "\n",
    "auto_woe.generate_report(report_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda_py38",
   "language": "python",
   "name": "anaconda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
